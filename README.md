# llms_code_generation_comparision
# ğŸ‘¨â€ğŸ’» Qualitative Analysis of LLM-generated and Human-written Code

This project explores and evaluates the quality, readability, and performance of code generated by Large Language Models (LLMs) such as ChatGPT and Gemini, compared to human-written code in **C++** and **Python**.


## ğŸ“Œ Project Summary

We analyzed 400+ code samples across two difficulty levels and measured four key metrics:

- âœ… Cyclomatic Complexity  
- âœ… Lines of Code  
- âœ… Time Complexity  
- âœ… Space Complexity  

A **custom interactive dashboard** was built to visualize and compare AI vs human code.



## ğŸ§  Research Questions

- **RQ1**: How do correctness, structure, and performance differ between human and LLM-generated code?
- **RQ2**: How does prompt specificity influence LLM output quality?
- **RQ3**: What drives user preference for AI vs human code?

---

## ğŸ–¥ï¸ Dashboard Preview


The dashboard provides:
- Code visualization
- Metric-based comparisons
- Survey insights

---

## ğŸ› ï¸ Tech Stack

| Frontend        | Backend         | DB        | Analysis Tools     |
|-----------------|-----------------|-----------|--------------------|
| HTML, CSS, JS   | Node.js, Express| MongoDB   | Custom scripts (C++, Python), Big-O Calculator |

---

## ğŸ“Š Key Findings

- Human-written code is more reliable for complex problems.
- LLMs perform better with clear, simple prompts.
- ChatGPT is slightly better for Python; Gemini for C++.



## ğŸ“ Dataset Structure

dataset/
â”œâ”€â”€ human/
â”‚ â”œâ”€â”€ level1/
â”‚ â””â”€â”€ level2/
â”œâ”€â”€ chatgpt/
â”‚ â”œâ”€â”€ c++/
â”‚ â””â”€â”€ python/
â””â”€â”€ gemini/
â”œâ”€â”€ c++/
â””â”€â”€ python/


1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/llm-vs-human-code-analysis.git
Install dependencies:
cd project_folder
npm install
Run the app:


Evaluate newer LLMs (e.g., GPT-4o, Claude)

ğŸ“¬ Contact
