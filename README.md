# llms_code_generation_comparision
# 👨‍💻 Qualitative Analysis of LLM-generated and Human-written Code

This project explores and evaluates the quality, readability, and performance of code generated by Large Language Models (LLMs) such as ChatGPT and Gemini, compared to human-written code in **C++** and **Python**.


## 📌 Project Summary

We analyzed 400+ code samples across two difficulty levels and measured four key metrics:

- ✅ Cyclomatic Complexity  
- ✅ Lines of Code  
- ✅ Time Complexity  
- ✅ Space Complexity  

A **custom interactive dashboard** was built to visualize and compare AI vs human code.



## 🧠 Research Questions

- **RQ1**: How do correctness, structure, and performance differ between human and LLM-generated code?
- **RQ2**: How does prompt specificity influence LLM output quality?
- **RQ3**: What drives user preference for AI vs human code?

---

## 🖥️ Dashboard Preview


The dashboard provides:
- Code visualization
- Metric-based comparisons
- Survey insights

---

## 🛠️ Tech Stack

| Frontend        | Backend         | DB        | Analysis Tools     |
|-----------------|-----------------|-----------|--------------------|
| HTML, CSS, JS   | Node.js, Express| MongoDB   | Custom scripts (C++, Python), Big-O Calculator |

---

## 📊 Key Findings

- Human-written code is more reliable for complex problems.
- LLMs perform better with clear, simple prompts.
- ChatGPT is slightly better for Python; Gemini for C++.



## 📁 Dataset Structure

dataset/
├── human/
│ ├── level1/
│ └── level2/
├── chatgpt/
│ ├── c++/
│ └── python/
└── gemini/
├── c++/
└── python/


1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/llm-vs-human-code-analysis.git
Install dependencies:
cd project_folder
npm install
Run the app:


Evaluate newer LLMs (e.g., GPT-4o, Claude)

📬 Contact
